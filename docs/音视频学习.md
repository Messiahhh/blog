

# 音视频学习

## 基本原理

### 帧率

视频的本质是连续播放的图片，帧率表示着一秒钟刷新的图片的帧数，帧率为24FPS以上时人脑就会觉得比较流畅了，因此一般的动画或者电影的帧率为24FPS或30FPS。

### 分辨率

即每帧图片的分辨率，如1080P的视频指的是1920*1080像素，每一个像素都由RGB三个通道构成，即使用3 * 8位来表达。

### 码率

码率指的是单位时间内传输的比特数，对于一个给定24帧率、1080P的视频来说，它的码率为24 * 1920 * 1080 * 3 * 8比特每秒，约等于142MB每秒，可见未压缩的视频容量是很大的，所以我们需要采用视频编码来对视频进行压缩。



### 视频编码

视频编码即去除数据中冗余信息的压缩技术，常见的视频编码格式有*H.264*、H.265、*MPEG-4*、VP8等。


- 软编码指的是使用CPU进行编码解码
- 硬编码指的是使用非CPU硬件（如GPU）进行编码解码，性能相对来说更高



### 音频编码

常见的音频编码格式有*WAV、MP3、AAC、AC-3*等。



### 封装格式

将编码后的视频、音频、以及如标题、时长、分辨率等信息打包再一起的文件，被称作封装格式（容器格式）。常见的视频封装格式包括MP4、MKV、WebM（基于MKV，主要视频编码格式为VP8、VP9）、QuickTime（即MOV）、FLV、AVI、TS等。


### M3U8



### 编解码流程

完整流程如下：音频和视频采集完成后分别进行音频编码和视频编码、对编码后的产物进行封装，传输后在客户端进行解封装从而分离出音频和视频，再分别进行解码得到原始的音频和视频，再进行音视频同步，最后进行播放。

![编解码流程](../static/img/视频.png)





## WebRTC

### MediaStream

利用浏览器的API我们能够获取屏幕、摄像机、麦克风，Video、Audio甚至Canvas元素的流数据。可以通过`srcObject`将流赋给媒体元素，也可以使用MediaRecorder实现录制功能。

``` js
const stream2 = new Media(stream.getVideoTracks());

video.srcObject = stream;
```



#### getDisplayMedia

```js
const stream = await navigator.mediaDevices.getDisplayMedia({
  video: true,
  audio: true
})
```

#### getUserMedia

``` js
const stream = navigator.mediaDevices.getUserMedia({
  video: true,
  audio: true
})
```

#### captureMedia

``` js
const stream = canvas.captureMedia()
const stream2 = video.captureMedia()
const stream3 = audio.captureMedia()
```





### MediaRecorder


## AudioContext
> TODO

## Media Source Extension

在HTML5中最简单的音视频播放方式是直接将资源链接传入*video*标签的*src*属性。这种加载方式也被称为流式加载，即并不需要等待资源完全下载好后才开始音视频的播放，而是一边加载数据一边进行播放。通过观察控制台可以发现，在加载一个大体积视频的时候，通常会发送多个请求来分段请求该资源，每个请求都会带着`Range: byteds=<start>-`的请求头，响应的状态码是206，响应头包括`Content-Length: <length>; Content-Range: byted <start>-<end>/<length>`。

上述方式是直接将加载后的视频数据交给*video*自动进行解封装、解码、播放，这种方式虽然实现简单但也限制了我们扩展其功能的能力，实际上目前主流的视频播放器都不是使用这种原生方式播放视频的，相反它们都是借助**媒体扩展（Media Source Extension）**来实现各种定制化的需求，比如可以实现以下特性：

1. HTML5本身不支持FLV、HLS（m3u8）容器格式，但我们可以先通过*fetch*或*ajax*手动请求资源数据，再将其转化为分段式MP4格式（FMP4），再通过MediaSource进行播放，从而实现对多种容器格式的兼容。
2. 请求视频资源时可以通过持续发送请求的形式来进行分段请求（如可以通过Range头部实现），并依次通过appendBuffer来实现流式加载，从而可以实现分辨率的动态切换、视频源的切换、直播功能。（以前想要实现分辨率的切换本质是修改*video*的*src*属性，重新加载新的视频源时可能会卡一下）

在介绍具体的实现之前，我们可以先观察一下主流的视频网站（Youtube、B站等）的播放器的行为。首先能够观察到它们的*video*标签的`src`并不是指向着一个真实存在的URL地址，而是一个类似`blob:https://www.bilibili.com/ed89dd41-bff1-427e-80c4-fa796d34c3cb`的虚拟URL；另外观察网络栏可以发现，在视频的播放过程中会持续发送请求来加载数据，一般来说每次请求的数据大小为几百KB。

接下来将会简单介绍MediaSource的API以及使用方式。

``` html
 <body>
    <video id="video"></video>
    <script>
        const video = document.getElementById("video");
        const mediaSource = new MediaSource();
        mediaSource.addEventListener("sourceopen", function () {
            const mimeCodec = 'video/mp4; codecs="avc1.42E01E, mp4a.40.2"';
            const sourceBuffer = mediaSource.addSourceBuffer(mimeCodec);
            fetch("http://server.com/my-video.mp4")
                .then((res) => res.arrayBuffer())
                .then((buf) => {
                    sourceBuffer.addEventListener("updateend", function () {
                        mediaSource.endOfStream();
                        video.play();
                    });
                    sourceBuffer.appendBuffer(buf);
                });
        });
        const url = URL.createObjectURL(mediaSource);
        video.src = url;
    </script>
</body>
```

首先通过`new MediaSource()`创建*mediaSource*实例，通过`URL.createObjectURL`创建虚拟地址，然后把虚拟地址传给*video*，这会触发*mediaSource*的`sourceopen`事件，此时我们可以创建一个或多个*sourceBuffer*，然后发送请求并把获取到的媒体资源通过`appendBuffer`进行加载。

在这个例子中我们只发送了一个请求来获取完整的多媒体数据，在实际的应用中会使用多个请求来获取分段数据。



参考：https://medium.com/canal-tech/how-video-streaming-works-on-the-web-an-introduction-7919739f7e1





## FFMPEG


### ffprobe

### ffplay
### 获取视频信息

可用来获取视频时长、码率、分辨率、视频编码、音频编码等信息

``` shell
ffmpeg -i test.mp4 -hide_banner

ffprobe test.mp4
```





### 转换格式

将mp4格式转成mov格式

``` shell
ffmpeg -i test.mp4 test.mov
```



提取音频数据

``` shell
ffmpeg -i test.mp4 test.mp3
```

