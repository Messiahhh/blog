# Canvas and WebGL

## Canvas
``` js
const canvas = document.querySelector('canvas');
const context = canvas.getContext('2d');
```

### toBlob

``` js
canvas.toBlob(blob => {
  const url = URL.createObjectURL(blob);
}, "image/jpeg", 1) // quality
```

### toDataURL

``` js
const dataUrl = canvas.toDataURL("image/jpeg", 1); // quality
```

### captureStream
获取Canvas的媒体流，从而实现Video预览或者媒体录制的能力。
``` js
const stream = canvas.captureStream();

// 预览能力 
video.srcObject = stream;

// 录制能力
const recorder = new MediaRecorder(stream);
```


### drawImage

Canvas提供了`drawImage`方法将不同的图像源绘制到我们的目标Canvas上，图像源包括Image、Video甚至另一个Canvas对象，以及后文会介绍的ImageBitMap。

``` js
ctx.drawImage(image, 0, 0)
ctx.drawImage(video, 0, 0)
ctx.drawImage(canvas2, 0, 0)
```

`drawImage`函数是个重载函数，有几种不同的用法。（注：下文中的`d`表示目标Canvas Destination，`s`表示图像源头Source）

1. `drawImage(source, dx, dy)`。简单的用法，以`(dx, dy)`为原点绘制目标图像。

2. `drawImage(source, dx, dy, dWidth, dHeight)`。和用法一类似，额外提供了`width`和`height`的参数允许我们调整所绘制的图像的大小，从而实现类似缩放的效果。

3. `drawImage(source, sx, sy, sWidth, sHeight, dx, dy, dWidth, dHeight)`。这个用法调整了参数的顺序，可用来裁剪数据源的部分区域进行绘制。

   ![](https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/drawImage/canvas_drawimage.jpg)



:::info
`drawImage`使用不同的图像源时的行为不同，性能上也略有差异，笔者在M2 Macbook Pro下，将CPU降速6倍下用15000*15000的图片进行性能测试后得出以下结论。
> [Demo链接](https://github.com/Messiahhh/canvas-perf-demo)

1. `drawImage(Image)`，JS线程API调用很快，渲染线程绘制上屏较慢（将近3秒，耗时主要集中在图像解码上）。
2. `drawImage(Canvas)`，JS线程API调用很快，渲染线程绘制上屏也很快。**推荐**。
3. `drawImage(OffscreenCanvas)`，JS线程API调用很慢（将近3秒，耗时主要集中在图像解码上，可能是浏览器内部机制的原因图像解码完成后该函数才返回），渲染线程绘制上屏快。事实发现`drawImage(image)`到OffscreenCanvas上时不会触发图像解码，后续在把OffscreenCanvas绘制到Canvas上才会触发图像解码，因此在离屏渲染的时候应该选择使用Canvas而不是OffscreenCanvas。
4. `drawImage(ImageBitmap)`。JS线程API调用很快，渲染线程绘制上屏也很快。和用例2差不多。**推荐**。

在例子2和例子3中，我们需要先通过drawImage把图片绘制到用来缓存的Canvas/OffscreenCanvas上，但我没有立刻同步地把缓存的Canvas绘制到我们的目标Canvas上，而是使用了一个定时器来确保先执行渲染线程，从而保证我们的Canvas图像源本身已经绘制完毕ready了。
此时整体执行顺序如下：`1. JS线程 drawImage(image) -> 2. 渲染线程 把图片绘制到Canvas上 -> 3. JS线程 drawImage(canvas) -> 4. 渲染线程 绘制Canvas到Canvas`。
因此我实际上测量的是3和4的总时长，这也是离屏渲染的常见场景————我已经提前缓存好了Canvas/图像源了，现在关心的是调用drawImage(canvas)时上屏所需要的时间。

在其他的一些场景下，我们会在JS线程调用了`drawImage(image)`后立刻调用`drawImage(canvas)`，相当于我们例子中把定时器去掉的效果。
此时整体的执行顺序如下：`1. JS线程 drawImage(image) -> 2. JS线程 drawImage(canvas) -> 3. 渲染线程`。其实这个行为和上面例子3 OffscreenCanvas是基本一致的，事实也证明此时会在步骤2的`drawImage(canvas)`
中耗时将近3秒用来图像转码。
:::

### getImageData/putImageData

通过`getImageData`可以直接拿到Canvas指定区域对应的原始像素数据。可以通过指定的数学转换实现不同的效果，比如Konva的高斯模糊等滤镜就是通过纯CPU计算实现的。

``` js
const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height)

for (let i = 0; i < imageData.data.length; i += 4) {
  	imageData.data[i] = 255; // red channel
    imageData.data[i + 1] = 0; // green channel
    imageData.data[i + 2] = 255; // blue channel
    imageData.data[i + 3] = 200; // alpha channel
}
ctx.putImageData(imageData, 0, 0)
```

:::caution
需要特别注意的是，`getImageData`和`putImageData`都是非常耗CPU的操作，容易造成长任务。除非我们确实需要对像素做一些操作，否则都应该使用`drawImage`等方法来进行绘制。
:::

:::info GPU/CPU Canvas
默认情况下Canvas的创建和绘制都是在GPU上的（硬件加速），当我们调用`getImageData`或者`putImageData`时本质都是GPU显存和CPU内存的读写数据，这是个比较耗费性能的操作。如果我们的Canvas存在很频繁的这类读写操作，可以考虑使用`willReadFrequently`标识，这样Canvas的绘制数据都会被存储在CPU内存中，减少读写操作的延时，但同时也会失去GPU硬件加速的能力。

``` js
const ctx = canvas.getContext('2d', {
  willReadFrequently: true
})
```

> **willReadFrequently**
> 
> A boolean value that indicates whether or not a lot of read-back operations are planned. This will force the use of a software (instead of hardware accelerated) 2D canvas and can save memory when calling getImageData() frequently.
:::


### OffscreenCanvas
我们的主页面存在着互斥的JS线程（通常也被称为主线程）和渲染线程。每当JS线程执行完一个JS任务，就会切到渲染线程执行渲染任务（即屏幕内容的绘制），绘制完后又回到JS线程执行下一个JS任务。
这也是为什么长任务会导致页面卡顿，一般为了保证画面稳定在60帧的刷新率，我们需要每16.6秒绘制一次，但考虑到渲染线程本身的执行也是需要时间的，所以最终留给我们的每个JS任务的执行时间是比这更短的。

一般对于**前后依赖的长时间JS同步任务**，有种优化手段是利用如`setTimeout`或者`Promise.then`的能力把任务拆分成多个任务来异步执行（JS中的异步指的是单线程异步），从而避免任务过长导致页面卡顿。但任务整体的耗时是不会减少的（甚至更长了）。

而对于**互相独立的同步任务（JS任务或者渲染任务）**，我们可以借助Web Worker的能力实现并行的计算和渲染。
Web Worker都有着自己的JS线程和渲染线程，它们的执行不会影响到我们主页面的执行和渲染。而为了利用到Web Worker的渲染线程，我们需要使用到Canvas，然而不幸的是Web Worker环境下无法访问到DOM元素，为了解决这个问题浏览器在Web Worker下提供了和DOM完全解耦的Canvas————OffscreenCanvas。

在上文中我们也简单提到过OffscreenCanvas，它本身的绘制能力和普通的Canvas基本一致，并没有神奇的魔法来提升我们的绘制性能。能够在Web Worker下执行，充分利用Web Worker的渲染线程能力，是它最大的亮点。比如我们可以使用下文将要介绍的`transferControlToOffscreen`，实现通过Web Worker的渲染线程绘制主页面的内容。

:::info 并行处理
Web Worker提供给我们并行计算和渲染的能力，当我们存在特别多互相独立的任务时，理论上我们可以创建无数个Web Worker来实现并行加速。但实际上Web Worker的创建开销是不小的（进程级别），所以并不现实。我们一般会借助GPU的能力，来处理各种并行计算的复杂场景。
:::

#### transferControlToOffscreen
在JS线程调用Canvas的`transferControlToOffscreen`方法可以生成一个OffscreenCanvas实例，同时会把自身上下文的所有权转移给该实例。

这意味着我们将无法直接访问Canvas的上下文，但通过OffscreenCanvas却可以拿到Canvas的上下文。而我们可以把OffscreenCanvas传递给Web Worker，即可通过在Web Woker中调用OffscreenCanvas的能力来间接绘制JS线程的Canvas。

``` js title="main.js"
const canvas = document.createElement('canvas')
canvas.width = 5000
canvas.height = 5000
document.body.appendChild(canvas)

const offscreenCanvas = canvas.transferControlToOffscreen()
const worker = new Worker('./worker.js')
worker.postMessage({ canvas: offscreenCanvas }, [offscreenCanvas])
``` 


``` js title="worker.js"
let canvas = null;
self.onmessage = function(evt) {
    if (evt.data.canvas) {
        canvas = evt.data.canvas;
        draw()
    }
}

function draw() {
    if (canvas) {
        const ctx = canvas.getContext('2d');
        ctx.fillStyle = 'pink'
        ctx.fillRect(0, 0, canvas.width, canvas.height);
        requestAnimationFrame(draw)
    }
}
```

#### transferToImageBitmap
除了上述的方法外，我们也可以直接在Web Worker中初始化OffscreenCanvas实例并进行绘制操作。而为了将绘制内容同步到JS线程的Canvas上，我们首先可能会想到`getImageData`但很明显这太耗性能了，又或者把OffscreenCanvas传递到JS线程但我Worker线程后面还要用到所以也不行。因此浏览器给OffscreenCanvas提供了`transferToImageBitmap`的能力来解决这个问题。

在前述章节中我们介绍过，`getImageData`的本质是把GPU显存中的数据写入到CPU内存中，存在不小的性能开销。而`transferToImageBitmap`可以简单理解成GPU显存到GPU显存的传递，即把OffscreenCanvas当前绘制的内容转移到另一块GPU显存空间中，性能是比较好的，并且此时如果再尝试通过`getImageData`读取OffscreenCanvas的数据会发现都已经被重置了。


### ImageBitmap

#### createImageBitMap
``` js
const image = new Image();
image.src = '';
image.onload = function() {
  createImageBitmap(image).then(bitMap => {
    ctx.drawImage(bitMap, 0, 0);
  })
}
```


### 绘制 API

#### 矩形绘制

- `fillRect()`
- `strokeRect()`
- `clearRect()`



#### 路径绘制

路径即为多个点的连线。

``` js
ctx.beginPath()

ctx.fill()
ctx.stroke()

ctx.closePath()
```



#### 坐标变换

- `ctx.translate(x, y)`
- `ctx.rotate(radians)`。将坐标系顺时针旋转，弧度制比如`ctx.rotate(Math.PI)`
- `ctx.scale(x, y)`
- `ctx.transform(a, b, c, d, e, f)`



#### save/restore

Canvas的2D上下文的本质是一个状态机，因此它还提供了保存和恢复当前状态的能力。

``` js
ctx.save()
ctx.tranlate(10, 10)
ctx.fillRect(0, 0, 100, 100)
ctx.restore()
```






## 渲染引擎

### 异步批量渲染

类似React中的`setState`，修改数据后会在下一个tick再进行渲染。

``` js
// konva实现
function batchDraw() {
    if (!this._waitingForDraw) {
      this._waitingForDraw = true;
      window.requestAnimFrame(() => {
        this.draw();
        this._waitingForDraw = false;
      });
    }
    return this;
}
```

### 离屏渲染

当我们谈论到离屏渲染的技术，总是容易和OffscreenCanvas搞混淆。事实上，OffscreenCanvas的作用就是为了让我们在Web Worker这类的环境下使用Canvas的能力，如果我们单纯在JS线程使用OffscreenCanvas，这和直接使用Canvas基本没有区别。

而所谓的离屏渲染，一般指的是我们除了在文档中用于绘制内容的Canvas外，额外创建新的Canvas来缓存绘制的内容。比如当某个虚拟节点要绘制的内容始终不变时，我们可以直接使用DrawImage来把离屏Canvas的内容进行绘制，从而省去了调用Canvas API来重复绘制相同内容的情况，实现性能的优化。



### 分层渲染

可以使用多个Canvas进行渲染，比如把静态的内容和动态的内容进行分离。



### 局部渲染/脏区检测

一般我们每次渲染时都会通过`clearRect`将画布的内容清空并进行全量绘制，一些渲染库中允许我们只重新渲染部分的内容实现性能优化。



### 包围盒与碰撞检测

- AABB包围盒
- OBB包围盒
- 分离轴定律

### 事件机制

- 取色值法
  - 实现简单；适合不规则图形
  - 需要额外绘制一份Canvas
- 几何法（引射线法）
  - 实现和计算复杂；适合规则图形



### 布局系统

实现类似Flex的布局能力。




### 性能优化

- 减少上下文切换


## WebGL

浏览器所提供的WebGL给予了我们图形绘制的能力，WebGL本质上基于OpenGL ES2.0，而后者实际上是OpenGL的一个精简子集，缺少了一部分的能力（如几何着色器等）。

近年来WebGL2的实现也逐渐稳定，它基于OpenGL ES3.0。

``` js
const canvas = document.querySelector('canvas');
const webgl = canvas.getContext('webgl');
const webgl2 = canvas.getContext('webgl2');
```

### 关键术语
> WebGL上下文有大量的方法，弄清楚这些方法背后做了什么可以帮助我们更好地理解WebGL。[可视化网站](https://webglfundamentals.org/webgl/lessons/resources/webgl-state-diagram.html?exampleId=triangle#no-help)

#### ARRAY_BUFFER_BINDING（gl.ARRAY_BUFFER）
ARRAY_BUFFER_BINDING表示WebGL上下文当前所绑定的GPU Buffer，默认为空。通过`gl.createBuffer()`可以创建多个GPU Buffer，并通过`gl.bindBuffer(gl.ARRAY_BUFFER)`来切换当前所绑定的Buffer，再通过`gl.bufferData(gl.ARRAY_BUFFER, xxx)`来写入数据到绑定的Buffer中。

``` js
const buffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, buffer);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array([1.0, 1.0]), this.gl.STATIC_DRAW);
```

#### FRAMEBUFFER_BINDING（gl.FRAMEBUFFER）
FRAMEBUFFER_BINDING表示WebGL上下文当前所绑定的帧缓冲对象，默认为当前Canvas对象（有的地方会叫做默认帧缓冲）。通过`gl.createFrameBuffer()`可以创建多个帧缓冲对象，并通过`gl.bindFrameBuffer(gl.FRAMEBUFFER, xxx)`来切换当前所绑定的帧缓冲对象，我们会在当前所绑定的帧缓冲上绘制画面。因此可以用来实现离屏渲染，比如可以把第一次绘制写入的帧缓冲作为第二次绘制的纹理，来实现多个后处理效果的串联。

``` js
const fbo = gl.createFrameBuffer();
gl.bindFramebuffer(gl.FRAMEBUFFER, fbo);
```

#### ACTIVE_TEXTURE（gl.TEXTURE_2D）
纹理的绑定类似上面两个，但稍微复杂一点。简单来说，WebGL上下文存在8个纹理单元（BINDING），默认选中第0个纹理单元。

通过`gl.activeTexture(gl['TEXTURE' + 单元号])`来设置当前选中的纹理单元，并通过`gl.bindTexture(gl.TEXTURE_2D, xxx)`来切换当前纹理单元所绑定的纹理数据，再通过`gl.texImage2D(gl.TEXTURE_2D, xxx)`来写入数据到目标纹理，一般我们还需要再通过`gl.texParameteri(gl.TEXTURE_2D)`来设置纹理的参数。

``` js
gl.activeTexture(gl.TEXTURE0) // 选中0号纹理单元
gl.activeTexture(gl.TEXTURE7) // 选中7号纹理单元 

const texture = gl.createTexture();
gl.bindTexture(gl.TEXTURE_2D, texture);
gl.texImage2D( // 写入数据到纹理
  gl.TEXTURE_2D,
  0,
  gl.RGBA,
  gl.RGBA,
  gl.UNSIGNED_BYTE,
  new Uint8Array([
   255, 0, 0, 255,
   0, 255, 0, 255,
  ])
);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR); // 设置纹理参数
```

#### VERTEX_ARRAY_BINDING

- 顶点缓冲对象（VBO）：一般用来保存顶点坐标、顶点颜色、UV信息、法线信息的Buffer也被称为顶点缓冲对象VBO。

- 顶点数组对象（VAO）：我们绘制的每一个顶点，都对应一次顶点着色器的执行，而顶点着色器中的Attribute的取值需要由我们在外部指定。简单来说WebGL上下文中会维护一个类似于Map的顶点数组对象（VAO），记录了每个Attribute应该从哪个Buffer（VBO）中以多大的步长（size）和偏移值（offset）取值。

``` js
const location = gl.getAttribLocation(program, 'akara');
gl.bindBuffer(gl.ARRAY_BUFFER, buffer);
gl.enableVertexAttribArray(location);
gl.vertexAttribPointer( // 这里有个隐式依赖，会把参数信息和当前绑定的gl.ARRAY_BUFFER一起注册到VAO中
    location,     // location
    3,            // size (components per iteration)
    gl.FLOAT,     // type of to get from buffer
    false,        // normalize
    0,            // stride (bytes to advance each iteration)
    0,            // offset (bytes from start of buffer)
);
```

#### CURRENT_PROGRAM
一个WebGL上下文可以存在多个程序，并通过`gl.useProgram()`来切换程序。这也是为什么类似`gl.getAttribLocation()`和`gl.getUniformLocation()`的方法的第一个参数是目标程序。

#### VIEWPORT
顶点着色器的作用是把顶点坐标重新映射到横纵-1到+1的裁剪空间上，我们需要通过设置Viewport来告诉WebGl我们的屏幕空间大小，从而WebGl内部会实现裁剪空间到屏幕空间的转换。
``` js
gl.viewport(0, 0, 100, 100)
```


### 渲染管线

1. 准备阶段。包括着色器的编译、程序的链接、VBO的创建和顶点数据的写入、设置VAO信息、设置Uniform、设置纹理数据等。
2. 渲染阶段。手动调用`gl.drawArrays()`发出绘制指令给CPU。
3. 我们绘制多少个顶点，就会执行多少次顶点着色器。顶点着色器的目的是把顶点坐标转换成齐次裁剪空间下的坐标。一般我们的顶点数据是通过3D建模软件导出，我们会先通过**模型矩阵Model**将模型空间转变为世界空间，再通过**视图矩阵View**将世界空间转变为观察空间，再通过**投影矩阵Projection（透视投影或正交投影）**转变为**齐次裁剪空间（Clip Space）**。以上的变换统称MVP变化。裁剪空间中坐标的XYZ三维都位于[-1,1]之间，W分量通常为1。
![](https://img-blog.csdn.net/20171229111837073?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYmllemhpaHVh/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
4. GPU内部通过透视除法将裁剪空间转换为**标准化设备坐标（NDC）**、再通过屏幕映射把-1到+1映射成我们的视口Viewport大小。再通过图元装配形成图形（如三角形），然后计算出图元所包含的像素区域，再会通过插值算出每个像素应该对应的Varying值用于后续像素着色器的计算。![img](https://img-blog.csdnimg.cn/b26e74a5ee5b4f8781cbe684ac25ebaa.png)
5. 逐像素执行像素着色器（片段着色器），输出的内容会写入到帧缓冲，显示器会定期读取来显示内容。
6. 深度测试等



### OpenGL Shading Language（GLSL）

#### Vertex Shader

``` glsl
// vertex shader 顶点着色器
attribute vec2 a_position;

void main() {
  gl_Position = vec4(a_position, 0.0, 1.0);
}
```
![vs](https://images2018.cnblogs.com/blog/669331/201803/669331-20180302113903143-870412752.png)



#### Fragment Shader

``` glsl
// fragment shader 片段着色器/像素着色器
precision mediump float;

void main() {
  gl_FragColor = vec4(1.0, 0.0, 0.0, 1.0); 
}
```
![fs](https://img-blog.csdn.net/20140917135608231)

#### attribute
`attribute`是顶点着色器的参数，运行时GPU会根据我们提前在VAO中的设置来从Buffer中读取。一般来说我们会使用`attribute`来记录顶点坐标、颜色、UV、法向量等信息。

#### varying
在顶点着色器中，我们除了会把数据写入到`gl_Position`外，可能还会把数据写入到`varying`类型的变量中。后续图元装配后，会通过插值计算出各个像素中对应的`varying`变量的值，然后这些值就会作为像素着色器的参数。

#### uniform
`uniform`可以被顶点着色器以及片段着色器访问，可以理解成纯粹的全局变量。它的值需要我们在JS代码中手动设置。
``` js
const location1 = gl.getUniformLocation(program, "u_1");
const location2 = gl.getUniformLocation(program, "u_2");

gl.uniform1f(location1, 1.);
gl.uniform1i(location2, 2);
```
#### texture
纹理也是一种特殊的全局变量`uniform`，在Shader程序中它的数据类型为`sampler2D`，它的设置方式也类似普通的`uniform`。
``` js
const texture = gl.createTexture();
const unit = 3 // 我们的例子中随便选了个3号单元
gl.activeTexture(gl['TEXTURE' + unit]) // 选中3号单元
gl.bindTexture(gl.TEXTURE_2D, texture) // 绑定纹理到3号单元
gl.texImage2D( // 写入数据到纹理
  gl.TEXTURE_2D,
  0,
  gl.RGBA,
  gl.RGBA,
  gl.UNSIGNED_BYTE,
  new Uint8Array([
   255, 0, 0, 255,
   0, 255, 0, 255,
  ])
);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR); // 设置纹理参数

const location = gl.getUniformLocation(program, 'u_texture');
gl.uniform1i(location, unit); // 设置纹理uniform时我们只需要告诉GPU纹理绑定在哪个纹理单元即可。
```

``` glsl
precision mediump float;
 
uniform sampler2D u_texture;
 
void main() {
   vec2 texcoord = vec2(0.5, 0.5); // 纹理的左下角为(0, 0)，右上角为(1, 1)，因此这里的(0.5, 0.5)为纹理中心点。一般这个uv值是通过varying传到像素着色器的。
   gl_FragColor = texture2D(u_texture, texcoord); // 纹理采样
}

```



#### 数据类型


#### 内置函数

- `abs(x)`，绝对值。
- `floor(x)`，获取整数部分。
- `fract(x)`，获取小数部分。
- `ceil(x)`，向上取整。
- `max(x, y)`，`min(x, y)`，`clamp(x, min, max)`
- `mix(a, b, t)`，混合（线性组合），`mix(a, b, t)`表示`a * (1 - t) + b * (t)`。
- `step(edge, x)`，当`x`小于`edge`时返回0，否则返回1。
- `smoothstep(edge0, edge1, x)`，当`x`小于`edge0`时返回0，当`x`大于`edge0`时返回1，否则返回的值为`edge0`到`edge1`的插值。
- `length(vec)`，返回向量的长度。





#### 纹理（Texture）、UV坐标与采样

把一张图片作为Texture纹理，图片左下角为(0, 0)，右上角为(1, 1)，可以通过UV坐标来进行采样。





#### drawArrays

`wegl`的实际绘制指令，我们实际上是绘制无数个顶点，绘制线段或三角形，本质上是在点与点之间进行线性插值。

``` js
gl.drawArrays(gl.POINTS, 0, 3) // 绘制三个顶点

gl.drawArrays(gl.LINES, 0, 2) // 绘制两个顶点，两个顶点之间的内容通过线性插值，因此最终看到的是线段

gl.drawArrays(gl.TRIANGLES, 0, 3) // 绘制三个顶点，三个顶点之间的内容通过线性插值，因此最终看到的是三角形

// 先绘制buffer中前三个顶点，再绘制后三个顶点，总共六个
gl.drawArrays(gl.TRIANGLES, 0, 3)
gl.drawArrays(gl.TRIANGLES, 3, 3)

```



### 随机数和噪声

噪声在图形学中存在着许多的应用，比如可以实现故障艺术。通常我们会借助随机数来生成噪声图，GLSL内置了`rand`这一确定性随机（即伪随机），但通常我们会自己实现一个伪随机函数。

``` glsl
y = fract(sin(x)*10000.0);
```

为了从二维向量生成随机数，以上的式子又可以被扩展成如下，式子中的魔法数字是经过实践后被广泛使用的数字。

``` glsl
float random (vec2 st) {
    return fract(sin(dot(st.xy, vec2(12.9898, 78.233))) * 43758.5453123);
}
```



### 混合模式

#### 溶解

> TODO





### 模糊效果

> 高斯模糊







### 线性代数
- 向量加法
``` glsl 
vec2 result = vec2(1.0, 1.0) + 2.
```

- 向量数乘
``` glsl 
vec2 result = vec2(1.0, 1.0) * 2.;
```

- 向量相加
``` glsl 
vec2 result = vec2(1.0, 1.0) + vec2(2.0, 2.0);
```

- 向量的线性组合
``` glsl 
vec2 result = vec2(1.0, 1.0) * 0.5 + vec2(2.0, 2.0) * 1;
vec2 result2 = vec(1.0, 1.0) * t + vec2(2.0, 2.0) * (1 - t) // 线性插值公式
```

- 向量内积（点积），得到向量A*向量B在A方向上的投影。
- 向量外积（乘积），得到两个向量的法向量

- 矩阵加法
``` glsl
mat2 result = mat2(1., 1., 1. 1.) + 0.5;
```
- 矩阵数乘
``` glsl
mat2 result = mat2(1., 1., 1. 1.) * 0.5;
```

- 矩阵相加
``` glsl
mat2 result = mat2(1., 1., 1. 1.) + mat2(1., 1., 1. 1.);
```

- 矩阵乘法：矩阵乘法本质是向量的线性组合，即对于C=AB，有C=B.map(b => 线性组合(Ab))
``` glsl
mat2 result = mat2(1., 1., 1. 1.) * mat2(1., 1., 1. 1.);
```

### Trouble Shooting

1. Shader代码记得加分号。

2. Shader代码中数字记得小数点。

3. 顶点顺序逆时针。

4. 图像绘制反了。

   ``` glsl
   this.gl.pixelStorei(this.gl.UNPACK_FLIP_Y_WEBGL, true);
   ```

5. 图像绘制黑屏，纹理宽高需要是二的幂。

   ``` glsl
   this.gl.texParameteri(
     this.gl.TEXTURE_2D,
     this.gl.TEXTURE_MIN_FILTER,
     this.gl.LINEAR
   );
   this.gl.texParameteri(
     this.gl.TEXTURE_2D,
     this.gl.TEXTURE_WRAP_S,
     this.gl.CLAMP_TO_EDGE
   );
   this.gl.texParameteri(
     this.gl.TEXTURE_2D,
     this.gl.TEXTURE_WRAP_T,
     this.gl.CLAMP_TO_EDGE
   );
   ```

6. WebGL内置8个纹理单元，绑定多个纹理时需要提前激活。

   ``` glsl
   gl.activeTexture(gl.TEXTURE0);
   ```

   











### 参考链接

1. https://juejin.cn/post/6891918137671811079

2. https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API/Tutorial
3. https://shaderific.com/glsl/common_functions.html
4. https://en.wikipedia.org/wiki/Blend_modes
